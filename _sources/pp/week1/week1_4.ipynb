{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running computations in parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a square and circle of radius one inside the square. \n",
    "\n",
    "Ratio between surface areas of circle and square\n",
    "\n",
    "$\\lambda = \\frac{\\pi}{4}$\n",
    "\n",
    "Estimating λ: \n",
    "randomly sample points inside the square\n",
    "\n",
    "Count how many fall inside the circle\n",
    "\n",
    "Multiply this ratio by 4 for an estimate of $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmcCount\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmonteCarloPiSeq\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "def mcCount(iter: Int): Int = {\n",
    "val randomX = new Random\n",
    "val randomY = new Random\n",
    "var hits = 0\n",
    "for (i <- 0 until iter) {\n",
    "val x = randomX.nextDouble // in [0,1]\n",
    "val y = randomY.nextDouble // in [0,1]\n",
    "if (x*x + y*y < 1) hits= hits + 1\n",
    "}\n",
    "hits\n",
    "}\n",
    "def monteCarloPiSeq(iter: Int): Double = 4.0 * mcCount(iter) / iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres3\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m3.1458\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monteCarloPiSeq(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the parallel version of computing the same estimate.\n",
    "\n",
    "```scala\n",
    "def monteCarloPiPar(iter: Int): Double = {\n",
    "val ((pi1, pi2), (pi3, pi4)) = parallel(\n",
    "parallel(mcCount(iter/4), mcCount(iter/4)),\n",
    "parallel(mcCount(iter/4), mcCount(iter - 3*(iter/4))))\n",
    "4.0 * (pi1 + pi2 + pi3 + pi4) / iter\n",
    "}\n",
    "```\n",
    "\n",
    "All computations are happening in parallel and independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Class Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now describe a more flexible construct for describing parallel computation. Consider\n",
    "\n",
    "```scala\n",
    "val (v1, v2) = parallel(e1, e2)\n",
    "```\n",
    "\n",
    "we can write alternatively using the `task` construct:\n",
    "\n",
    "```scala\n",
    "val t1 = task(e1)\n",
    "val t2 = task(e2)\n",
    "val v1 = t1.join\n",
    "val v2 = t2.join\n",
    "t1\n",
    "t2\n",
    "\n",
    "```\n",
    "\n",
    "`\n",
    "t = task(e)` starts computation `e` \"in the background\"\n",
    "\n",
    "* `t` is a task, which performs computation of `e`\n",
    "* current computation proceeds in parallel with `t`\n",
    "* to obtain the result of `e`, use `t.join`\n",
    "* `t.join` blocks and waits until the result is computed\n",
    "* subsequent `t.join` calls quickly return the `s`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a minimal interface for tasks:\n",
    "\n",
    "```scala\n",
    "def task(c: => A) : Task[A]\n",
    "trait Task[A] {\n",
    "def join: A\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "`task` and `join` establish maps between computations and tasks\n",
    "In terms of the value computed the equation `task(e).join==e` holds\n",
    "\n",
    "We can omit writing `.join` if we also define an implicit conversion.\n",
    "\n",
    "```scala\n",
    "implicit def getJoin[T](x:Task[T]): T = x.join\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen four-way parallel p-norm:\n",
    "\n",
    "```scala\n",
    "val ((part1, part2),(part3,part4)) =\n",
    "parallel(parallel(sumSegment(a, p, 0, mid1),\n",
    "sumSegment(a, p, mid1, mid2)),\n",
    "parallel(sumSegment(a, p, mid2, mid3),\n",
    "sumSegment(a, p, mid3, a.length)))\n",
    "power(part1 + part2 + part3 + part4, 1/p)\n",
    "```\n",
    "\n",
    "Here is essentially the same computation expressed using task:\n",
    "```scala\n",
    "val t1 = task {sumSegment(a, p, 0, mid1)}\n",
    "val t2 = task {sumSegment(a, p, mid1, mid2)}\n",
    "val t3 = task {sumSegment(a, p, mid2, mid3)}\n",
    "val t4 = task {sumSegment(a, p, mid3, a.length)}\n",
    "power(t1 + t2 + t3 + t4, 1/p)\n",
    "```\n",
    "\n",
    "Notice the implicit conversion of `t1`, `t2`, `t3` and `t4` in the last statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are allowed to use task\n",
    "Implement parallel construct as a method using task\n",
    "\n",
    "```scala\n",
    "def parallel[A, B](cA: => A, cB: => B): (A, B) = {\n",
    "val tB: Task[B] = task { cB }\n",
    "val tA: A = cA\n",
    "(tA, tB.join)\n",
    "}\n",
    "```\n",
    "\n",
    "Suppose we want to compute computation `A` and `B`. Using `task` construct start a computation in parallel, and we continue in parallel\n",
    "in thread of this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "What is wrong with the following definitions?\n",
    "\n",
    "```scala\n",
    "def parallelWrong[A, B](cA: => A, cB: => B): (A, B) = {\n",
    "val tB: B = (task { cB }).join\n",
    "val tA: A = cA\n",
    "(tA, tB.join)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, above defintion compiles it does not the benefit of parallel computation in as task `tB` completed (immediatly called join) before task `tA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Parallel Programs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* testing – ensures that parts of the program are behaving according to the intended behavior\n",
    "* benchmarking – computes performance metrics for parts of the program.\n",
    "\n",
    "By contrast benchmarking is used to evaluate various evaluation metrics of the program.\n",
    "\n",
    "A performance metric could be\n",
    "\n",
    "* running time\n",
    "* memory foot print\n",
    "* network traffic\n",
    "* disk usage\n",
    "* latency \n",
    "\n",
    "Often, this value is a random variable, it's value varies from run to run.\n",
    "For example if we want to test the runnning time on list reverse we could "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mxs\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n",
       "\u001b[36mstartTime\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m37832158629139L\u001b[39m\n",
       "\u001b[36mres11_2\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xs = List(1,2,3)\n",
    "val startTime = System.nanoTime\n",
    "xs.reverse\n",
    "println((System.nanoTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very naive way but we start with this to illustrate the purpose.\n",
    "\n",
    "Typically, testing yields a binary output – a program or its part is either correct or it is not.\n",
    "\n",
    "Benchmarking usually yields a continuous value, which denotes the extent to which the program is correct.\n",
    "\n",
    "In our benchmarking example running time is a continous value and will be slightly different each time.\n",
    "\n",
    "## Why do we benchmark parallel programs?\n",
    "\n",
    "Performance benefits are the main reason why we are writing parallel\n",
    "programs in the first place.\n",
    "\n",
    "Benchmarking parallel programs is even more important than benchmarking sequential programs.\n",
    "\n",
    "In sequential program the only measure performance are bottlenecks of the system. Converselt,a parallel program is typically a bottleneck in the system to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance (specifically, running time) is subject to many factors:\n",
    "* processor speed\n",
    "* number of processors\n",
    "* memory access latency and throughput (affects contention)\n",
    "* cache behavior (e.g. false sharing, associativity effects)\n",
    "* runtime behavior (e.g. garbage collection, JIT compilation, thread scheduling)\n",
    "\n",
    "\n",
    "The faster the processor the faster the resulting program. \n",
    "\n",
    "A parallel program to some extent distribute some of its workload across different processors. The number of available processors is necessary pre-condition to improve the performance. \n",
    "\n",
    "Since processors are separated from main memory with a bus they have to wait while fetching data from memory. Here we differentiate latency, which is the amount of time processor must wait since it requested data from main memory until data arrives, throughput the amount of data can be retrieved from memory per unit time.\n",
    "\n",
    "These two effect memory contention.To decrease latency,high speed memory, called cache close to the processor cores which mirrors the main memory. Cache are divided into several levels. It allows to read data with out going to the bus. It boost performance by several orders of magnitude. \n",
    "\n",
    "However caches makes performance analysis complicated. Caches can decrease peformance (false sharing).\n",
    "\n",
    "Almost all programs run within some runtime environment, such as VM, MMU or OS. These components use garbage collection, JIT compilation or thread scheduling. \n",
    "\n",
    "In reality many others drive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring performance is difficult – usually, the a performance metric is a\n",
    "random variable.\n",
    "* multiple repetitions\n",
    "* statistical treatment – computing mean and variance\n",
    "* eliminating outliers\n",
    "* ensuring steady state (warm-up)\n",
    "* preventing anomalies (GC, JIT compilation, aggressive optimizations)\n",
    "\n",
    "garbage collection can be avoided by allocating heap memory.\n",
    "JIT compilation can turned off.\n",
    "Compilers can make optimizations.\n",
    "\n",
    "To learn more, see _Statistically Rigorous Java Performance Evaluation, by Georges, Buytaert, and Eeckhout_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScalaMeter\n",
    "\n",
    "ScalaMeter is a benchmarking and performance regression testing\n",
    "framework for the JVM.\n",
    "* performance regression testing – comparing performance of the current program run against known previous runs\n",
    "* benchmarking – measuring performance of the current (part of the) program\n",
    "\n",
    "We will focus on benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, add ScalaMeter as a dependency.\n",
    "\n",
    "```scala\n",
    "libraryDependencies +=\n",
    "\"com.storm-enroute\" %% \"scalameter-core\" % \"0.6\"\n",
    "```\n",
    "Then, import the contents of the ScalaMeter package, and measure:\n",
    "\n",
    "```scala\n",
    "import org.scalameter._\n",
    "val time = measure {\n",
    "(0 until 1000000).toArray\n",
    "}\n",
    "println(s\"Array initialization time: $time ms\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the above snippet in Scala REPl,we get numbers with a lot of variance. During the run JVM could have started dynamic compilation, evidence for this shorter time in few runs. Another thing could have potentially occured is garabage collection, during the runs more and more memory allocated. When it reaches certain threshold garbage collection kicks in. It happens periodically. Running time could be high for garbage collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demo showed two very different running times on two consecutive\n",
    "runs of the program.\n",
    "\n",
    "When a JVM program starts, it undergoes a period of warmup, after\n",
    "which it achieves its maximum performance.\n",
    "\n",
    "* first, the program is interpreted (the program is run in interpreter, the byte code output of Scala compiler runs directly on interpreter)\n",
    "* then, parts of the program are compiled into machine code (JVM is smart enough to figure out which parts are hot parts (frequently used) , and those parts are compiled and turned into machine code\n",
    "* later, the JVM may choose to apply additional dynamic optimizations\n",
    "* eventually, the program reaches steady state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we want to measure steady state program performance.\n",
    "\n",
    "ScalaMeter `Warmer` objects run the benchmarked code until detecting\n",
    "steady state.\n",
    "\n",
    "```scala\n",
    "import org.scalameter._\n",
    "val time = withWarmer(new Warmer.Default) measure {\n",
    "(0 until 1000000).toArray\n",
    "}\n",
    "```\n",
    "\n",
    "ScalaMeter configuration clause allows specifying various parameters, such as the minimum and maximum number of warmup runs.\n",
    "```scala\n",
    "val time = config(\n",
    "Key.exec.minWarmupRuns -> 20,\n",
    "Key.exec.maxWarmupRuns -> 60,\n",
    "Key.verbose -> true\n",
    ") withWarmer(new Warmer.Default) measure {\n",
    "(0 until 1000000).toArray\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, ScalaMeter can measure more than just the running time.\n",
    "\n",
    "* Measurer.Default – plain running time\n",
    "* IgnoringGC – running time without GC pauses\n",
    "* OutlierElimination – removes statistical outliers\n",
    "* MemoryFootprint – memory footprint of an object\n",
    "* GarbageCollectionCycles – total number of GC pauses\n",
    "* newer ScalaMeter versions can also measure method invocation counts and boxing counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the memory footprint\n",
    "```scala\n",
    "withMeasurer(new Measurer.MemoryFootprint) withWarmer( new Warmer.Default)  measure { (0 until 100000).toArray}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
